import csv
import time
import re
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# ìˆ˜ì§‘í•  ì¹´í…Œê³ ë¦¬ ì •ë³´ ì„¤ì •
CATEGORIES = {
    "ëª¨ë°”ì¼": "https://news.naver.com/breakingnews/section/105/731",
    "ì¸í„°ë„· & SNS": "https://news.naver.com/breakingnews/section/105/226",
    "í†µì‹  & ë‰´ë¯¸ë””ì–´": "https://news.naver.com/breakingnews/section/105/227",
    "IT ì¼ë°˜": "https://news.naver.com/breakingnews/section/105/230",
    "ì»´í“¨í„°": "https://news.naver.com/breakingnews/section/105/283",
    "ê³¼í•™ ì¼ë°˜": "https://news.naver.com/breakingnews/section/105/228"
}

def setup_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

def clean_text(text):
    """ì œëª©ì—ì„œ í•œê¸€ê³¼ ê³µë°±ë§Œ ë‚¨ê¸°ê³  ì œê±° (ìœ ì‚¬ë„ ì¸¡ì • ì •í™•ë„ í–¥ìƒ)"""
    return re.sub(r'[^ê°€-í£\s]', '', text)

def deduplicate_articles(data_list, threshold=0.4):
    """TF-IDFì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì´ìš©í•œ ì¤‘ë³µ ê¸°ì‚¬ ì œê±°"""
    if not data_list:
        return []

    df = pd.DataFrame(data_list, columns=['ë¶„ë¥˜', 'ì œëª©', 'ì‹œê°„', 'ë§í¬'])
    final_indices = []

    # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬ ìˆ˜í–‰
    for category in df['ë¶„ë¥˜'].unique():
        category_df = df[df['ë¶„ë¥˜'] == category].copy()
        if len(category_df) <= 1:
            final_indices.extend(category_df.index.tolist())
            continue

        # 1. í…ìŠ¤íŠ¸ ì •ì œ ë° ë²¡í„°í™”
        titles = category_df['ì œëª©'].apply(clean_text).tolist()
        vectorizer = TfidfVectorizer(min_df=1)
        tfidf_matrix = vectorizer.fit_transform(titles)
        
        # 2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
        
        # 3. ìœ ì‚¬ë„ ê¸°ë°˜ í•„í„°ë§
        keep_mask = [True] * len(category_df)
        for i in range(len(category_df)):
            if not keep_mask[i]: continue
            for j in range(i + 1, len(category_df)):
                # ì„¤ì •í•œ threshold(0.4)ë³´ë‹¤ ë†’ìœ¼ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
                if cosine_sim[i, j] > threshold:
                    keep_mask[j] = False
        
        final_indices.extend(category_df.iloc[keep_mask].index.tolist())

    return df.loc[final_indices].values.tolist()

def collect_section_news(driver, category_name, url):
    print(f"\nğŸ“‚ [{category_name}] ì„¹ì…˜ ìˆ˜ì§‘ ì‹œì‘...")
    driver.get(url)
    news_data = []
    seen_links = set()
    found_yesterday = False

    while not found_yesterday:
        articles = driver.find_elements(By.CLASS_NAME, "sa_item")
        if not articles: break

        for article in articles:
            try:
                dt_element = article.find_element(By.CSS_SELECTOR, ".sa_text_datetime b")
                time_text = dt_element.text.strip()

                if "1ì¼ì „" in time_text:
                    print(f"   âœ‹ '1ì¼ì „' ê¸°ì‚¬ ë„ë‹¬. [{category_name}] ì¢…ë£Œ.")
                    found_yesterday = True
                    break

                title_element = article.find_element(By.CLASS_NAME, "sa_text_title")
                title = title_element.text.strip()
                link = title_element.get_attribute("href")

                if link not in seen_links:
                    news_data.append([category_name, title, time_text, link])
                    seen_links.add(link)
            except: continue

        if found_yesterday: break

        try:
            more_button = driver.find_element(By.CLASS_NAME, "section_more_inner")
            if more_button.is_displayed():
                more_button.click()
                time.sleep(1.5)
            else: break
        except: break
            
    return news_data

def save_to_csv(all_data):
    if not all_data:
        print("\nâŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    filename = "naver_today_news.csv"
    with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow(['ë¶„ë¥˜', 'ì œëª©', 'ì‹œê°„', 'ë§í¬'])
        writer.writerows(all_data)
    print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename} (ì´ {len(all_data)}ê±´)")

if __name__ == "__main__":
    driver = setup_driver()
    raw_news = []

    try:
        # 1. ëª¨ë“  ì¹´í…Œê³ ë¦¬ ìˆœíšŒí•˜ë©° ìˆ˜ì§‘
        for category, url in CATEGORIES.items():
            raw_news.extend(collect_section_news(driver, category, url))
        
        print(f"\n--- ìˆ˜ì§‘ ì™„ë£Œ (ì´ {len(raw_news)}ê±´) ---")
        
        # 2. ìì—°ì–´ ì²˜ë¦¬ë¡œ ìœ ì‚¬ ì œëª© ì œê±°
        print("ğŸ¤– AI ì¤‘ë³µ í•„í„°ë§ ì‘ë™ ì¤‘...")
        filtered_news = deduplicate_articles(raw_news, threshold=0.4)
        print(f"âœ¨ í•„í„°ë§ ê²°ê³¼: {len(raw_news)}ê±´ -> {len(filtered_news)}ê±´ìœ¼ë¡œ ì••ì¶•")
        
        # 3. ìµœì¢… ê²°ê³¼ ì €ì¥
        save_to_csv(filtered_news)
        
    finally:
        driver.quit()
