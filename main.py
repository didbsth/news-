import csv
import time
import re
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# 1. ìˆ˜ì§‘í•  ì¹´í…Œê³ ë¦¬ ì„¤ì •
CATEGORIES = {
    "ëª¨ë°”ì¼": "https://news.naver.com/breakingnews/section/105/731",
    "ì¸í„°ë„· & SNS": "https://news.naver.com/breakingnews/section/105/226",
    "í†µì‹  & ë‰´ë¯¸ë””ì–´": "https://news.naver.com/breakingnews/section/105/227",
    "IT ì¼ë°˜": "https://news.naver.com/breakingnews/section/105/230",
    "ì»´í“¨í„°": "https://news.naver.com/breakingnews/section/105/283",
    "ê³¼í•™ ì¼ë°˜": "https://news.naver.com/breakingnews/section/105/228"
}

def setup_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

def clean_text(text):
    """ìœ ì‚¬ë„ ì¸¡ì •ì„ ìœ„í•´ í•œê¸€/ê³µë°±ë§Œ ë‚¨ê¹€"""
    return re.sub(r'[^ê°€-í£\s]', '', text)

def filter_ai_keywords(data_list):
    """ì œëª©ì— 'AI', 'ai', 'ì¸ê³µì§€ëŠ¥'ì´ í¬í•¨ëœ ê¸°ì‚¬ë§Œ 1ì°¨ ì¶”ì¶œ"""
    filtered_data = []
    pattern = re.compile(r'ai|ì¸ê³µì§€ëŠ¥', re.IGNORECASE)
    
    for item in data_list:
        title = item[1]
        if pattern.search(title):
            filtered_data.append(item)
    return filtered_data

def deduplicate_articles(data_list, threshold=0.2):
    """ì¶”ì¶œëœ AI ê¸°ì‚¬ë“¤ ì¤‘ ìœ ì‚¬í•œ ì œëª© ì œê±° (ê¸°ì¤€ 0.2)"""
    if not data_list:
        return []

    df = pd.DataFrame(data_list, columns=['ë¶„ë¥˜', 'ì œëª©', 'ì‹œê°„', 'ë§í¬'])
    final_indices = []

    for category in df['ë¶„ë¥˜'].unique():
        category_df = df[df['ë¶„ë¥˜'] == category].copy()
        if len(category_df) <= 1:
            final_indices.extend(category_df.index.tolist())
            continue

        titles = category_df['ì œëª©'].apply(clean_text).tolist()
        vectorizer = TfidfVectorizer(min_df=1)
        tfidf_matrix = vectorizer.fit_transform(titles)
        cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
        
        keep_mask = [True] * len(category_df)
        for i in range(len(category_df)):
            if not keep_mask[i]: continue
            for j in range(i + 1, len(category_df)):
                if cosine_sim[i, j] > threshold:
                    keep_mask[j] = False
        
        final_indices.extend(category_df.iloc[keep_mask].index.tolist())

    return df.loc[final_indices].values.tolist()

def collect_section_news(driver, category_name, url):
    print(f"\nğŸ“‚ [{category_name}] ì„¹ì…˜ ìˆ˜ì§‘ ì‹œì‘...")
    driver.get(url)
    news_data = []
    seen_links = set()
    found_yesterday = False

    while not found_yesterday:
        articles = driver.find_elements(By.CLASS_NAME, "sa_item")
        if not articles: break

        for article in articles:
            try:
                dt_element = article.find_element(By.CSS_SELECTOR, ".sa_text_datetime b")
                time_text = dt_element.text.strip()
                if "1ì¼ì „" in time_text:
                    found_yesterday = True
                    break

                title_element = article.find_element(By.CLASS_NAME, "sa_text_title")
                title = title_element.text.strip()
                link = title_element.get_attribute("href")

                if link not in seen_links:
                    news_data.append([category_name, title, time_text, link])
                    seen_links.add(link)
            except: continue

        if found_yesterday: break
        try:
            more_button = driver.find_element(By.CLASS_NAME, "section_more_inner")
            more_button.click()
            time.sleep(1.5)
        except: break
            
    return news_data

def save_to_csv(all_data):
    if not all_data:
        print("\nâŒ ìµœì¢… ê²°ê³¼ê°€ ì—†ì–´ íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    filename = "naver_today_news.csv"
    with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow(['ë¶„ë¥˜', 'ì œëª©', 'ì‹œê°„', 'ë§í¬'])
        writer.writerows(all_data)
    print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename} (ìµœì¢… {len(all_data)}ê±´)")

if __name__ == "__main__":
    driver = setup_driver()
    raw_news = []

    try:
        # 1. ë‰´ìŠ¤ ì „ì²´ ìˆ˜ì§‘
        for category, url in CATEGORIES.items():
            raw_news.extend(collect_section_news(driver, category, url))
        
        print(f"\n--- 1ë‹¨ê³„: ìˆ˜ì§‘ ì™„ë£Œ ({len(raw_news)}ê±´) ---")
        
        # 2. AI ê´€ë ¨ ê¸°ì‚¬ 1ì°¨ í•„í„°ë§ (ìˆœì„œ ë³€ê²½ë¨)
        print("ğŸ” 2ë‹¨ê³„: AI ê´€ë ¨ ê¸°ì‚¬ ì¶”ì¶œ ì¤‘...")
        ai_news = filter_ai_keywords(raw_news)
        
        # 3. í•„í„°ë§ëœ ê²°ê³¼ ë‚´ì—ì„œ ì¤‘ë³µ ì œê±° (threshold 0.2)
        print(f"ğŸ¤– 3ë‹¨ê³„: ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° ì¤‘ (ê¸°ì¤€: 0.2, ëŒ€ìƒ: {len(ai_news)}ê±´)...")
        final_news = deduplicate_articles(ai_news, threshold=0.2)
        
        print(f"\nâœ¨ ìµœì¢… ìš”ì•½: ì „ì²´({len(raw_news)}ê±´) -> AIì¶”ì¶œ({len(ai_news)}ê±´) -> ì¤‘ë³µì œê±°({len(final_news)}ê±´)")
        
        # 4. ì €ì¥
        save_to_csv(final_news)
        
    finally:
        driver.quit()
